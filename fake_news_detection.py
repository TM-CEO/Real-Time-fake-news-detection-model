# -*- coding: utf-8 -*-
"""Fake news detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CaroOmCyr-UeKDCrQLjfi97a9yNG6_2W
"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers import LSTM, Dense
from transformers import TFBertForSequenceClassification
import pandas as pd
import numpy as np
!pip install pandas transformers sklearn nltk
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
import torch
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

data=pd.read_csv('/content/Data.csv')
data.head()

# Check for null values in the dataset
null_values = data.isnull().sum()

# Display the columns with null values
print("Null Values in Each Column:")
print(null_values[null_values > 0])

# Drop rows with any null values
df_clean = data.dropna()

# Verify that null values have been removed
print(f"Rows after dropping nulls: {df_clean.shape[0]}")

# Drop rows where 'YourColumn' has null values
df = df_clean.dropna(subset=['text'])

# Display the filtered dataset
print(df.head())

# Optionally, check the number of rows remaining after dropping nulls
print(f"Rows remaining after dropping nulls: {df.shape[0]}")
df.head()

# Check for null values in the dataset
null_values = df.isnull().sum()

# Display the columns with null values
print("Null Values in Each Column:")
print(null_values[null_values > 0])

### Tokenization - using BERT ###

# Initialize BERT tokenizer
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the text column
df['tokenized_text'] = df['text'].apply(lambda x: bert_tokenizer.tokenize(x))

### Lematization ###

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Apply lemmatization on the tokenized text
df['lemmatized_text'] = df['tokenized_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])

### Stop word Removal ###

# Get English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from the lemmatized text
df['cleaned_text'] = df['lemmatized_text'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])

### Feature Extraction - for traditional ML models ###

# Convert the cleaned tokens back to text format for TF-IDF vectorization
df['cleaned_text_str'] = df['cleaned_text'].apply(lambda tokens: ' '.join(tokens))

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the cleaned text to generate TF-IDF features
tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_text_str'])

# Convert to DataFrame
tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Display the TF-IDF feature DataFrame
tfidf_df.head()

### Feature Extraction - for DL using BERT ###

# Initialize BERT model
bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)

# Function to get BERT embeddings
def get_bert_embeddings(text):
    # Tokenize input text
    inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    # Get hidden states from BERT
    with torch.no_grad():
        outputs = bert_model(**inputs)
        hidden_states = outputs.hidden_states

    # Get the embeddings from the last hidden state
    embeddings = hidden_states[-1]
    return torch.mean(embeddings, dim=1).squeeze().numpy()

# Apply BERT embedding extraction on the cleaned text
df['bert_embeddings'] = df['cleaned_text_str'].apply(get_bert_embeddings)

# Create a new DataFrame for the embeddings
bert_embeddings_df = pd.DataFrame(df['bert_embeddings'].to_list())

# Display the BERT embeddings DataFrame
bert_embeddings_df.head()

# Import the necessary library
from sklearn.model_selection import train_test_split

# Split dataset into features (X) and labels (y)
X = df['text']
y = df['label']

# Split into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the training data, and transform the test data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, roc_auc_score
import seaborn as sns
import time
from transformers import BertTokenizer, TFBertForSequenceClassification
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Split the dataset into features (X) and labels (y)
X = df['text'].tolist()
y = df['label'].tolist()

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize BERT tokenizer
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the data (BERT requires a specific input format)
train_encodings = bert_tokenizer(X_train, truncation=True, padding=True, max_length=128, return_tensors='tf')
test_encodings = bert_tokenizer(X_test, truncation=True, padding=True, max_length=128, return_tensors='tf')

# Load pre-trained BERT model for binary classification
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Compile the model with Adam optimizer and sparse categorical crossentropy loss (since this is a binary classification task)
# Use the 'adam' string to specify the optimizer instead of passing the optimizer object directly
bert_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Measure training latency
start_train = time.time()

# Train the model
history = bert_model.fit(
    train_encodings['input_ids'], np.array(y_train),
    validation_data=(test_encodings['input_ids'], np.array(y_test)),
    epochs=3,
    batch_size=16
)

end_train = time.time()
train_latency = end_train - start_train
print(f"Training Latency: {train_latency:.4f} seconds")

# Measure prediction latency
start_predict = time.time()

# Get predictions on the test set
y_pred_probs = bert_model.predict(test_encodings['input_ids'])
y_pred = np.argmax(y_pred_probs.logits, axis=1)

end_predict = time.time()
predict_latency = end_predict - start_predict
print(f"Prediction Latency: {predict_latency:.4f} seconds")

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
class_report = classification_report(y_test, y_pred, target_names=['Fake', 'Real'])
print(class_report)

# Precision-recall curve
y_scores = tf.nn.softmax(y_pred_probs.logits, axis=1)[:,1].numpy()  # Get probabilities
precision, recall, thresholds = precision_recall_curve(y_test, y_scores)

# Plot the precision-recall curve
plt.figure(figsize=(6,4))
plt.plot(recall, precision, marker='.', label='BERT')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_scores)

# Plot the ROC curve
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, marker='.', label='BERT (AUC = %0.2f)' % roc_auc_score(y_test, y_scores))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()